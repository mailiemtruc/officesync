import uvicorn
import httpx
import datetime
import logging
from typing import Optional, Dict, List
from contextlib import asynccontextmanager
from fastapi import FastAPI
from pydantic import BaseModel
from pydantic_settings import BaseSettings, SettingsConfigDict
import google.generativeai as genai
from google.ai.generativelanguage_v1beta.types import content

# Import Manager
from tool_manager import manager
# Import service ng√¥n ng·ªØ
import services.language as lang_service 

# --- C·∫§U H√åNH ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Settings(BaseSettings):
    GOOGLE_API_KEY: str
    ATTENDANCE_SERVICE_URL: str
    
    model_config = SettingsConfigDict(env_file=".env", extra="ignore")

settings = Settings()
genai.configure(api_key=settings.GOOGLE_API_KEY)

# --- B·ªò NH·ªö CHAT (RAM) ---
CHAT_HISTORY: Dict[int, List] = {}

# --- LIFESPAN ---
http_client: Optional[httpx.AsyncClient] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global http_client
    http_client = httpx.AsyncClient(timeout=30.0)
    yield
    await http_client.aclose()

app = FastAPI(lifespan=lifespan)

class ChatRequest(BaseModel):
    userId: int
    message: str

@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    try:
        # 1. Context c∆° b·∫£n
        today = datetime.date.today()
        service_instructions = manager.get_combined_prompts()
        user_lang = lang_service.USER_PREFERENCES.get(req.userId)
        
        common_rules = """
        QUY T·∫ÆC ·ª®NG X·ª¨:
        1. Th√°i ƒë·ªô: L·ªÖ ph√©p, Nh·∫π nh√†ng, Chuy√™n nghi·ªáp.
        2. ANTI-ROBOT: KH√îNG b·∫Øt ƒë·∫ßu b·∫±ng "OK", "Ok". D√πng "D·∫° v√¢ng", "V√¢ng", "Th∆∞a b·∫°n".
        """

        if not user_lang:
            lang_instruction = f"""
            ‚ö†Ô∏è TR·∫†NG TH√ÅI: Ng∆∞·ªùi d√πng M·ªöI (Ch∆∞a l∆∞u thi·∫øt l·∫≠p ng√¥n ng·ªØ).
            {common_rules}
            
            NHI·ªÜM V·ª§: T·ª± ƒë·ªông nh·∫≠n di·ªán v√† l∆∞u ng√¥n ng·ªØ.

            K·ªäCH B·∫¢N H√ÄNH ƒê·ªòNG:
            1. N·∫øu User CH√ÄO ho·∫∑c n√≥i "START_CONVERSATION":
               -> H·ªèi l·ªãch s·ª±: "B·∫°n mu·ªën giao ti·∫øp b·∫±ng English hay Ti·∫øng Vi·ªát?".
            
            2. N·∫øu User H·ªéI TH·∫≤NG v√†o nghi·ªáp v·ª• (VD: "Ch·∫•m c√¥ng ch∆∞a?", "Attendance history", "T√¥i ƒëi tr·ªÖ kh√¥ng"):
               -> B∆Ø·ªöC 1: Ph√¢n t√≠ch ng√¥n ng·ªØ User ƒëang d√πng (Vietnamese hay English).
               -> B∆Ø·ªöC 2: G·ªåI NGAY tool `set_language` v·ªõi ng√¥n ng·ªØ ƒë√≥. (QUAN TR·ªåNG: Ph·∫£i g·ªçi tool n√†y ƒë·ªÉ h·ªá th·ªëng ghi nh·ªõ).
               -> B∆Ø·ªöC 3: Sau ƒë√≥ m·ªõi g·ªçi ti·∫øp c√°c tool ch·∫•m c√¥ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.
            
            3. N·∫øu User n√≥i t√™n ng√¥n ng·ªØ (VD: "Ti·∫øng Vi·ªát", "vn", "English"):
               -> G·ªçi `set_language` v√† x√°c nh·∫≠n.
            """
        else:
            if user_lang == "Vietnamese":
                greeting_guide = "Xin ch√†o! Ch√†o m·ª´ng b·∫°n quay tr·ªü l·∫°i OfficeSync. T√¥i c√≥ th·ªÉ h·ªó tr·ª£ g√¨ cho c√¥ng vi·ªác c·ªßa b·∫°n h√¥m nay?"
            else:
                greeting_guide = "Welcome back to OfficeSync! How can I assist you with your work today?"

            lang_instruction = f"""
            ‚úÖ TR·∫†NG TH√ÅI: Ng√¥n ng·ªØ {user_lang}.
            {common_rules}
            K·ªäCH B·∫¢N:
            - N·∫øu User ch√†o ho·∫∑c n√≥i "START_CONVERSATION" -> {greeting_guide}
            - N·∫øu User ƒëang tr·∫£ l·ªùi c√¢u h·ªèi tr∆∞·ªõc ƒë√≥ (V√≠ d·ª•: "C√≥", "Kh√¥ng", "Chi ti·∫øt ƒëi") -> H√ÉY TI·∫æP T·ª§C M·∫†CH TRUY·ªÜN, ƒê·ª™NG CH√ÄO L·∫†I.
            """

        # 2. System Prompt
        full_system_instruction = f"""
        Th·ªùi gian hi·ªán t·∫°i: {today.strftime('%Y-%m-%d')}.
        B·∫°n l√† tr·ª£ l√Ω ·∫£o OfficeSync. UserID hi·ªán t·∫°i: {req.userId}.
        
        --- ƒêI·ªÄU KHI·ªÇN NG√îN NG·ªÆ ---
        {lang_instruction}
        
        --- H∆Ø·ªöNG D·∫™N NGHI·ªÜP V·ª§ ---
        {service_instructions}
        """

        # 3. Kh·ªüi t·∫°o Model
        model = genai.GenerativeModel(
            'gemini-2.0-flash', 
            tools=manager.tools_schema,
            system_instruction=full_system_instruction 
        )

        user_history = CHAT_HISTORY.get(req.userId, [])
        chat = model.start_chat(history=user_history, enable_automatic_function_calling=False)

        # 4. G·ª≠i tin nh·∫Øn User
        response = await chat.send_message_async(req.message)

        # --- X·ª¨ L√ù SONG SONG (BATCH PROCESSING) ---
        final_text = ""
        
        while True:
            # A. T√¨m T·∫§T C·∫¢ c√°c Function Call
            function_calls = []
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    if part.function_call and part.function_call.name:
                        function_calls.append(part.function_call)

            # B. N·∫øu c√≥ Function Call
            if function_calls:
                response_parts = []
                
                # Th·ª±c thi T·ª™NG tool
                for fc in function_calls:
                    tool_name = fc.name
                    args = {k: v for k, v in fc.args.items()}
                    
                    logger.info(f"ü§ñ Tool Call: {tool_name} | Args: {args}")

                    try:
                        tool_result = await manager.handle_tool_call(
                            tool_name, req.userId, args, http_client, settings
                        )
                    except Exception as e:
                        tool_result = f"Error executing tool: {str(e)}"

                    response_parts.append(
                        genai.protos.Part(
                            function_response=genai.protos.FunctionResponse(
                                name=tool_name,
                                response={"result": tool_result}
                            )
                        )
                    )

                # C. G·ª≠i k·∫øt qu·∫£ v·ªÅ Gemini
                response = await chat.send_message_async(
                    genai.protos.Content(parts=response_parts)
                )
                continue 
            
            else:
                # --- KH√îNG G·ªåI TOOL (Ch·ªâ l√† Text) ---
                final_text = response.text
                break 

        # 6. L∆∞u l·ªãch s·ª≠
        CHAT_HISTORY[req.userId] = chat.history

        # [QUAN TR·ªåNG] Th√™m .strip() ƒë·ªÉ c·∫Øt b·ªè d√≤ng tr·ªëng th·ª´a ·ªü cu·ªëi
        return {"reply": final_text.strip() if final_text else ""}

    except Exception as e:
        logger.error(f"Error: {e}", exc_info=True)
        return {"reply": "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë x·ª≠ l√Ω y√™u c·∫ßu."}

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=5000, reload=True)